# src/core/llm_state_machine.py
"""
MÃ¡quina de estados que guÃ­a a cada LLM SOTA a travÃ©s del proceso completo
de anÃ¡lisis y resoluciÃ³n de vulnerabilidades.
"""

import json
from enum import Enum, auto
from typing import Dict, Any, Optional, List
from dataclasses import dataclass, field
from datetime import datetime
import asyncio
import os


class LLMState(Enum):
    """Estados de la mÃ¡quina de estados de un LLM"""
    IDLE = auto()              # Esperando trabajo
    ANALYZING = auto()         # Analizando vulnerabilidad
    TEST_DESIGNING = auto()    # DiseÃ±ando test conceptual
    FIX_DESIGNING = auto()     # DiseÃ±ando fix
    DOCUMENTING = auto()       # Documentando soluciÃ³n
    WAITING_COUNCIL = auto()   # Esperando discusiÃ³n del consejo
    COUNCIL_DISCUSSING = auto() # Discutiendo en consejo
    FINISHED = auto()          # Trabajo completado
    ERROR = auto()             # Error en algÃºn estado


@dataclass
class LLMStateContext:
    """Contexto que se mantiene a travÃ©s de los estados"""
    vulnerability: Dict[str, Any]
    worktree_dir: str
    current_file: str
    analysis_result: Optional[Dict] = None
    test_design: Optional[Dict] = None
    fix_code: Optional[Dict] = None
    documentation: Optional[Dict] = None
    council_feedback: Optional[List] = None
    errors: List[str] = field(default_factory=list)
    
    def add_error(self, error: str):
        """AÃ±ade un error al contexto"""
        self.errors.append(f"{datetime.now().isoformat()}: {error}")


class LLMStateMachine:
    """
    MÃ¡quina de estados que guÃ­a a un LLM a travÃ©s del proceso completo.
    
    Estados:
    1. ANALYZING â†’ Comprender vulnerabilidad
    2. TEST_DESIGNING â†’ DiseÃ±ar test que demuestre vulnerabilidad
    3. FIX_DESIGNING â†’ DiseÃ±ar soluciÃ³n
    4. DOCUMENTING â†’ Documentar soluciÃ³n
    5. COUNCIL_DISCUSSING â†’ Participar en consejo
    6. FINISHED â†’ Completado
    """
    
    def __init__(self, llm, llm_name: str, worktree_manager, council=None):
        self.llm = llm
        self.llm_name = llm_name
        self.worktree_manager = worktree_manager
        self.council = council
        
        self.state = LLMState.IDLE
        self.context: Optional[LLMStateContext] = None
        self.state_history = []
        self.start_time = None
        self.end_time = None
        self._current_task = None
        
    async def execute_workflow(self, vulnerability: Dict[str, Any], 
                               worktree_dir: str, branch_name: str) -> Dict[str, Any]:
        """
        Ejecuta el flujo completo de trabajo para una vulnerabilidad.
        
        Args:
            vulnerability: Dict con informaciÃ³n de la vulnerabilidad del SAST
            worktree_dir: Directorio del worktree asignado
            branch_name: Nombre de la rama de trabajo
            
        Returns:
            Resultado completo con todos los artefactos generados
        """
        self.start_time = datetime.now()
        self.context = LLMStateContext(
            vulnerability=vulnerability,
            worktree_dir=worktree_dir,
            current_file=vulnerability.get('file', 'unknown')
        )
        
        print(f"\nðŸ¤– [{self.llm_name}] Iniciando workflow en rama '{branch_name}'...")
        
        try:
            # Estado 1: ANALYZING
            await self._transition_to(LLMState.ANALYZING)
            await self._state_analyzing()
            
            # Estado 2: TEST_DESIGNING
            await self._transition_to(LLMState.TEST_DESIGNING)
            await self._state_test_designing()
            
            # Estado 3: FIX_DESIGNING
            await self._transition_to(LLMState.FIX_DESIGNING)
            await self._state_fix_designing()
            
            # Estado 4: DOCUMENTING
            await self._transition_to(LLMState.DOCUMENTING)
            await self._state_documenting()
            
            # Estado 5: COUNCIL_DISCUSSING (si hay consejo)
            if self.council:
                await self._transition_to(LLMState.WAITING_COUNCIL)
                await self._transition_to(LLMState.COUNCIL_DISCUSSING)
                await self._state_council_discussing(branch_name)
            
            # Estado final: FINISHED
            await self._transition_to(LLMState.FINISHED)
            
        except Exception as e:
            error_msg = f"Error en workflow: {str(e)}"
            print(f"âŒ [{self.llm_name}] {error_msg}")
            self.context.add_error(error_msg)
            await self._transition_to(LLMState.ERROR)
        
        self.end_time = datetime.now()
        return self._compile_results(branch_name)
    
    async def _transition_to(self, new_state: LLMState):
        """TransiciÃ³n entre estados con logging."""
        old_state = self.state
        self.state = new_state
        self.state_history.append({
            'timestamp': datetime.now().isoformat(),
            'from': old_state.name,
            'to': new_state.name
        })
        
        print(f"   ðŸ”„ [{self.llm_name}] {old_state.name} â†’ {new_state.name}")
    
    async def _state_analyzing(self):
        """Estado 1: Analizar y comprender la vulnerabilidad."""
        print(f"   ðŸ” [{self.llm_name}] Analizando vulnerabilidad...")
        
        try:
            # Preparar contexto enriquecido del worktree
            full_context = await self.worktree_manager.prepare_llm_context(
                issue_id=self.context.vulnerability.get('rule_id', 'unknown'),
                sast_issue=self.context.vulnerability,
                include_related=True
            )
            
            # Construir prompt de anÃ¡lisis
            analysis_prompt = self._build_analysis_prompt(full_context)
            
            # Llamar al LLM
            response = await self.llm.generate_response(
                prompt=analysis_prompt,
                temperature=0.1,
                max_tokens=2000
            )
            
            # Parsear respuesta JSON
            try:
                analysis_result = json.loads(response)
                if isinstance(analysis_result, str):
                    # Algunos LLMs devuelven JSON como string dentro de string
                    analysis_result = json.loads(analysis_result)
            except:
                # Si no es JSON vÃ¡lido, crear estructura bÃ¡sica
                analysis_result = {
                    "analysis": {
                        "what": "AnÃ¡lisis no estructurado",
                        "why_vulnerable": "No se pudo parsear respuesta",
                        "exploitation_scenario": "N/A",
                        "potential_impact": "desconocido",
                        "possible_solutions": []
                    },
                    "raw_response": response[:500]
                }
            
            self.context.analysis_result = analysis_result
            print(f"   âœ… [{self.llm_name}] AnÃ¡lisis completado")
            
        except Exception as e:
            error_msg = f"Error en anÃ¡lisis: {str(e)}"
            self.context.add_error(error_msg)
            raise
    
    def _build_analysis_prompt(self, full_context: Dict) -> str:
        """Construye el prompt para anÃ¡lisis de vulnerabilidad."""
        vuln = self.context.vulnerability
        
        prompt = f"""# ANÃLISIS DE VULNERABILIDAD DE SEGURIDAD

## INFORMACIÃ“N DE LA VULNERABILIDAD:
- **ID**: {vuln.get('rule_id', 'Unknown')}
- **Severidad**: {vuln.get('severity', 'Unknown')}
- **Confianza**: {vuln.get('confidence', 'Unknown')}
- **DescripciÃ³n**: {vuln.get('message', 'No description')}
- **Archivo**: {vuln.get('file', 'Unknown')}:{vuln.get('line', 0)}
- **CWE**: {vuln.get('cwe', 'N/A')}
- **OWASP**: {vuln.get('owasp', 'N/A')}

## CÃ“DIGO VULNERABLE:
```{self._get_file_extension(vuln.get('file', ''))}
{full_context.get('code', {}).get('vulnerable_section', '// No se pudo extraer cÃ³digo')}
```

## CONTEXTO ADICIONAL:
{full_context.get('code', {}).get('surrounding_code', '// No hay contexto adicional')}

## INSTRUCCIONES:
Analiza la vulnerabilidad reportada. Determina si es un falso positivo o un riesgo real.
Si es real, identifica exactamente por quÃ© es vulnerable y cÃ³mo podrÃ­a ser explotada.

Responde ÃšNICAMENTE en formato JSON con la siguiente estructura:
{{
    "analysis": {{
        "what": "Breve descripciÃ³n de la vulnerabilidad",
        "why_vulnerable": "ExplicaciÃ³n tÃ©cnica de la debilidad",
        "exploitation_scenario": "CÃ³mo un atacante podrÃ­a aprovecharla",
        "potential_impact": "Impacto en el sistema",
        "possible_solutions": ["soluciÃ³n 1", "soluciÃ³n 2"]
    }},
    "is_false_positive": false,
    "confidence_score": 0.9
}}
"""
        return prompt

    def _get_file_extension(self, filename: str) -> str:
        """Obtiene la extensiÃ³n del archivo para el bloque de cÃ³digo."""
        _, ext = os.path.splitext(filename)
        return ext.lstrip('.') if ext else ''

    async def _state_test_designing(self):
        """Estado 2: DiseÃ±ar un test que demuestre la vulnerabilidad."""
        print(f"   ðŸ§ª [{self.llm_name}] DiseÃ±ando test de concepto...")
        # ImplementaciÃ³n pendiente
        pass

    async def _state_fix_designing(self):
        """Estado 3: DiseÃ±ar el fix para la vulnerabilidad."""
        print(f"   ðŸ› ï¸ [{self.llm_name}] DiseÃ±ando soluciÃ³n...")
        # ImplementaciÃ³n pendiente
        pass

    async def _state_documenting(self):
        """Estado 4: Documentar la soluciÃ³n."""
        print(f"   ðŸ“ [{self.llm_name}] Documentando soluciÃ³n...")
        # ImplementaciÃ³n pendiente
        pass

    async def _state_council_discussing(self, branch_name: str):
        """Estado 5: Participar en el consejo de sabios."""
        print(f"   ðŸ›ï¸ [{self.llm_name}] Participando en el consejo...")
        # ImplementaciÃ³n pendiente
        pass

    def _compile_results(self, branch_name: str) -> Dict[str, Any]:
        """Compila todos los resultados del workflow."""
        return {
            "llm_name": self.llm_name,
            "branch": branch_name,
            "vulnerability": self.context.vulnerability,
            "analysis": self.context.analysis_result,
            "test_design": self.context.test_design,
            "fix": self.context.fix_code,
            "documentation": self.context.documentation,
            "errors": self.context.errors,
            "duration_seconds": (self.end_time - self.start_time).total_seconds() if self.end_time else 0
        }
