# üìã **ARCHIVO DE PROGRESO - TDH ENGINE**

## üìÑ **PROGRESS.md**

```markdown
# üöÄ Test-Driven Hardening (TDH) Engine - Progress Report

## üìÖ Date: February 5, 2025
## üéØ Current Status: Phase 1 Infrastructure Complete

## ‚úÖ **ACHIEVEMENTS - PHASE 1 COMPLETED**

### 1. **Enhanced SAST Analysis System**
- **Smart C++ filtering**: Reduced false positives from 13 to 0 in complex C++ repositories
- **Multi-tool integration**: cppcheck, bandit, semgrep, flawfinder, safety
- **Context enrichment**: Vulnerability context extraction for LLMs (CWE, code snippets, remediation hints)
- **Noise reduction**: Filters for protobuf files, namespace/class detection, BPF macros

### 2. **Git Worktree Architecture**
- **Single clone + isolated worktrees**: Each LLM gets its own workspace
- **Automatic branch management**: Branch creation, checkout, and cleanup
- **GitHub integration**: PAT authentication, remote operations, PR creation
- **Parallel execution**: Multiple LLMs can work simultaneously without conflicts

### 3. **LLM Council System**
- **6 SOTA models via OpenRouter**: Claude-3.5-Sonnet, GPT-4-Turbo, DeepSeek-Coder, Gemini-Pro, Llama-3.1-70B, CodeLlama-70B
- **Availability checking**: Automated detection of available LLMs
- **Odd-number selection**: Ensures "voting" capability for collaborative decisions
- **Asynchronous communication**: Prepared for inter-LLM discussions

### 4. **End-to-End Pipeline**
- **Complete flow**: Repo ‚Üí Clone ‚Üí Worktrees ‚Üí LLM Context ‚Üí Fix Generation ‚Üí PRs
- **Structured context**: Vulnerability data + code context + remediation guidelines
- **Full isolation**: Each LLM operates in its own environment
- **Automated cleanup**: Temporary directories and branches managed automatically

## üîç **CURRENT CHALLENGES**

1. **Mock vulnerability file**: LLMs receive `src/vulnerable.c` which doesn't exist in real repos
2. **Real SAST integration**: Need to connect LLM pipeline with actual SAST findings
3. **Test generation cycle**: Missing: vulnerability ‚Üí test proof ‚Üí discussion ‚Üí fix workflow
4. **Inter-LLM communication**: Council discussion and voting system not yet implemented

## üéØ **VISION FOR NEXT PHASE**

### **Complete Vulnerability Resolution Flow:**
```
1. Real SAST Analysis ‚Üí Identify actual vulnerabilities
2. For each CRITICAL vulnerability:
   a. Create LLM Council with odd number of LLMs
   b. Each LLM in isolated worktree:
      - Generates test proving vulnerability
      - Executes test (must FAIL)
      - Generates fix with documentation
      - Executes test with fix (must PASS)
      - Presents solution to council
   c. Council discussion and voting
   d. Engine creates PRs for each LLM solution
```

### **Key Metrics for Success:**
- [ ] Each LLM generates compilable, executable test proving vulnerability
- [ ] Tests demonstrate actual security impact
- [ ] LLMs collaborate to improve test quality
- [ ] Fixes resolve vulnerabilities without breaking functionality
- [ ] PRs include: fix + tests + documentation + security metrics

## üèóÔ∏è **TECHNICAL ARCHITECTURE STATUS**

### **Core Components Working:**
- `tdh_unified.py`: Main CLI with SAST analysis and LLM council commands
- `sast_orchestrator.py`: Enhanced SAST with C++ filtering and context enrichment
- `git_worktree_manager.py`: Complete Git operations with GitHub integration
- `llm_council.py`: LLM orchestration and availability management
- `openrouter_adapter.py`: API communication with multiple SOTA models

### **Configuration Files:**
- `config/llm_council.yaml`: LLM model configuration and priorities
- `config/sast_tools.yaml`: SAST tool configurations and filters

## üöÄ **IMMEDIATE NEXT STEPS**

### **Priority 1: Real SAST ‚Üí LLM Integration**
- Connect SAST findings with LLM worktree creation
- Use actual vulnerable files from repository
- Extract relevant code context for each vulnerability

### **Priority 2: Test Generation System**
- Prompt engineering for vulnerability test creation
- Test execution framework in worktrees
- Test result validation and reporting

### **Priority 3: Council Collaboration**
- Inter-LLM communication channel
- Solution presentation and discussion protocol
- Voting system for best solutions

## üìä **TEST RESULTS**

### **SAST Filtering Success:**
- Repository: `alonsoir/test-zeromq-c-`
- Before filtering: 13 CRITICAL issues (mostly false positives)
- After filtering: 0 CRITICAL issues (false positives eliminated)
- Real vulnerabilities in test files still detected ‚úÖ

### **LLM Council Availability:**
- 6/6 LLMs available via OpenRouter
- 3 LLMs successfully assigned to worktrees
- Context preparation working (needs real files)

### **Git Operations:**
- Repository cloning: ‚úÖ Working with PAT authentication
- Worktree creation: ‚úÖ Each LLM gets isolated directory
- Branch management: ‚úÖ Automatic creation and cleanup

## üîß **KNOWN ISSUES**

1. **File path mismatch**: Mock `src/vulnerable.c` doesn't exist in target repos
2. **Timeout handling**: Git operations sometimes timeout (needs optimization)
3. **Error recovery**: Partial failures need better cleanup
4. **Cost management**: LLM API usage tracking needed

## üìà **ROADMAP**

### **Phase 2 (Next Session):**
- Integrate real SAST findings with LLM pipeline
- Implement test generation and execution
- Basic council discussion protocol

### **Phase 3:**
- Advanced inter-LLM collaboration
- Solution quality evaluation metrics
- Automated PR review and merging criteria

### **Phase 4:**
- Multi-repository scaling
- Performance optimization
- Production deployment ready

## üë• **COLLABORATION MODEL**

The TDH Engine implements a novel "Council of LLMs" approach:
- **Odd number of LLMs** for voting capability
- **Isolated work environments** to prevent contamination
- **Collaborative improvement** through discussion
- **Comparative solutions** for human review

## üéâ **CONCLUSION**

**Phase 1 Infrastructure is complete and functional.** We have:
- ‚úÖ Working SAST analysis with smart filtering
- ‚úÖ Isolated Git worktree architecture
- ‚úÖ Multi-LLM council with SOTA models
- ‚úÖ End-to-end pipeline from repo to PRs

**Next session** will focus on connecting the dots: real vulnerabilities ‚Üí test proof ‚Üí collaborative fixing ‚Üí PR generation.

---

*Last Updated: February 5, 2025*
*Project Status: Active Development - Phase 1 Complete*
```

# üèóÔ∏è **M√ÅQUINA DE ESTADOS PARA CADA LLM SOTA - Dise√±o**

Tienes raz√≥n absoluta. Necesitamos una **m√°quina de estados formal** que gu√≠e a cada LLM a trav√©s del proceso completo. Y s√≠, recuerdo perfectamente el **ISSUE 003** donde t√∫ manualmente coordinabas la compilaci√≥n, testing y fixes entre diferentes LLMs. Esa es exactamente la experiencia que queremos automatizar.

## üéØ **ARQUITECTURA COMPLETA:**

```
TDH ENGINE
‚îú‚îÄ‚îÄ ü§ñ LLM COUNCIL (Consejo)
‚îÇ   ‚îú‚îÄ‚îÄ üß† LLM SOTA 1
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üèóÔ∏è State Machine (M√°quina de Estados)
‚îÇ   ‚îú‚îÄ‚îÄ üß† LLM SOTA 2
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üèóÔ∏è State Machine
‚îÇ   ‚îî‚îÄ‚îÄ üß† LLM SOTA 3
‚îÇ       ‚îî‚îÄ‚îÄ üèóÔ∏è State Machine
‚îú‚îÄ‚îÄ üîß Git Worktree Manager
‚îú‚îÄ‚îÄ üîç SAST Analyzer
‚îî‚îÄ‚îÄ üìä Orchestrator (Coordinador)
```

## üèóÔ∏è **DISE√ëO DE LA M√ÅQUINA DE ESTADOS:**

```python
# src/core/llm_state_machine.py
from enum import Enum, auto
from typing import Dict, Any, Optional, List
from dataclasses import dataclass
from datetime import datetime
import asyncio


class LLMState(Enum):
    """Estados de la m√°quina de estados de un LLM"""
    IDLE = auto()              # Esperando trabajo
    ANALYZING = auto()         # Analizando vulnerabilidad
    TEST_DESIGNING = auto()    # Dise√±ando test conceptual
    FIX_DESIGNING = auto()     # Dise√±ando fix
    DOCUMENTING = auto()       # Documentando soluci√≥n
    WAITING_COUNCIL = auto()   # Esperando discusi√≥n del consejo
    COUNCIL_DISCUSSING = auto() # Discutiendo en consejo
    FINISHED = auto()          # Trabajo completado
    ERROR = auto()             # Error en alg√∫n estado


@dataclass
class LLMStateContext:
    """Contexto que se mantiene a trav√©s de los estados"""
    vulnerability: Dict[str, Any]
    worktree_dir: str
    current_file: str
    analysis_result: Optional[Dict] = None
    test_design: Optional[Dict] = None
    fix_code: Optional[Dict] = None
    documentation: Optional[Dict] = None
    council_feedback: Optional[List] = None
    errors: List[str] = None
    
    def __post_init__(self):
        if self.errors is None:
            self.errors = []


class LLMStateMachine:
    """
    M√°quina de estados que gu√≠a a un LLM a trav√©s del proceso completo.
    
    Estados:
    1. ANALYZING ‚Üí Comprender vulnerabilidad
    2. TEST_DESIGNING ‚Üí Dise√±ar test que demuestre vulnerabilidad
    3. FIX_DESIGNING ‚Üí Dise√±ar soluci√≥n
    4. DOCUMENTING ‚Üí Documentar soluci√≥n
    5. COUNCIL_DISCUSSING ‚Üí Participar en consejo
    6. FINISHED ‚Üí Completado
    """
    
    def __init__(self, llm, llm_name: str, worktree_manager, council=None):
        self.llm = llm
        self.llm_name = llm_name
        self.worktree_manager = worktree_manager
        self.council = council
        
        self.state = LLMState.IDLE
        self.context: Optional[LLMStateContext] = None
        self.state_history = []
        self.start_time = None
        self.end_time = None
        
    async def execute_workflow(self, vulnerability: Dict[str, Any], 
                               worktree_dir: str, branch_name: str) -> Dict[str, Any]:
        """
        Ejecuta el flujo completo de trabajo para una vulnerabilidad.
        
        Returns:
            Resultado completo con todos los artefactos generados
        """
        self.start_time = datetime.now()
        self.context = LLMStateContext(
            vulnerability=vulnerability,
            worktree_dir=worktree_dir,
            current_file=vulnerability.get('file', 'unknown')
        )
        
        print(f"\nü§ñ [{self.llm_name}] Iniciando workflow...")
        
        try:
            # Estado 1: ANALYZING
            await self._transition_to(LLMState.ANALYZING)
            await self._state_analyzing()
            
            # Estado 2: TEST_DESIGNING
            await self._transition_to(LLMState.TEST_DESIGNING)
            await self._state_test_designing()
            
            # Estado 3: FIX_DESIGNING
            await self._transition_to(LLMState.FIX_DESIGNING)
            await self._state_fix_designing()
            
            # Estado 4: DOCUMENTING
            await self._transition_to(LLMState.DOCUMENTING)
            await self._state_documenting()
            
            # Estado 5: COUNCIL_DISCUSSING (si hay consejo)
            if self.council:
                await self._transition_to(LLMState.COUNCIL_DISCUSSING)
                await self._state_council_discussing(branch_name)
            
            # Estado final: FINISHED
            await self._transition_to(LLMState.FINISHED)
            
        except Exception as e:
            await self._transition_to(LLMState.ERROR)
            self.context.errors.append(str(e))
            print(f"‚ùå [{self.llm_name}] Error en workflow: {e}")
        
        self.end_time = datetime.now()
        return self._compile_results(branch_name)
    
    async def _transition_to(self, new_state: LLMState):
        """Transici√≥n entre estados con logging."""
        old_state = self.state
        self.state = new_state
        self.state_history.append({
            'timestamp': datetime.now(),
            'from': old_state.name,
            'to': new_state.name
        })
        
        print(f"   üîÑ [{self.llm_name}] {old_state.name} ‚Üí {new_state.name}")
    
    async def _state_analyzing(self):
        """Estado 1: Analizar y comprender la vulnerabilidad."""
        print(f"   üîç [{self.llm_name}] Analizando vulnerabilidad...")
        
        # Preparar contexto enriquecido
        full_context = self.worktree_manager.prepare_llm_context(
            branch_name="temp",  # Temporal, se actualizar√°
            sast_issue=self.context.vulnerability,
            include_related=True
        )
        
        # Prompt para an√°lisis
        analysis_prompt = f"""AN√ÅLISIS DE VULNERABILIDAD DE SEGURIDAD

VULNERABILIDAD:
- ID: {self.context.vulnerability.get('rule_id', 'Unknown')}
- Severidad: {self.context.vulnerability.get('severity', 'Unknown')}
- Descripci√≥n: {self.context.vulnerability.get('message', 'No description')}
- Archivo: {self.context.vulnerability.get('file', 'Unknown')}:{self.context.vulnerability.get('line', 0)}

C√ìDIGO VULNERABLE:
```
{full_context['code']['vulnerable_section']}
```

TAREA DE AN√ÅLISIS:
1. Explica QU√â hace el c√≥digo vulnerable
2. Identifica POR QU√â es vulnerable (CWE, tipo de ataque)
3. Describe C√ìMO un atacante podr√≠a explotarlo
4. Estima el IMPACTO potencial
5. Sugiere POSIBLES SOLUCIONES (alto nivel)

Formato de respuesta JSON:
{{
  "analysis": {{
    "what": "descripci√≥n del c√≥digo",
    "why_vulnerable": "explicaci√≥n t√©cnica",
    "exploitation_scenario": "c√≥mo se explota",
    "potential_impact": "alto/medio/bajo",
    "possible_solutions": ["soluci√≥n 1", "soluci√≥n 2"]
  }}
}}"""
        
        try:
            response = await self.llm.generate_fix({
                'vulnerability': self.context.vulnerability,
                'custom_prompt': analysis_prompt
            })
            
            # Parsear respuesta
            import json
            analysis_result = json.loads(list(response.values())[0])
            self.context.analysis_result = analysis_result
            
            print(f"   ‚úÖ [{self.llm_name}] An√°lisis completado")
            
        except Exception as e:
            print(f"   ‚ùå [{self.llm_name}] Error en an√°lisis: {e}")
            raise
    
    async def _state_test_designing(self):
        """Estado 2: Dise√±ar test conceptual que demuestre la vulnerabilidad."""
        print(f"   üß™ [{self.llm_name}] Dise√±ando test conceptual...")
        
        test_prompt = f"""DISE√ëO DE TEST CONCEPTUAL PARA DEMOSTRAR VULNERABILIDAD

CONTEXTO DE AN√ÅLISIS:
{self.context.analysis_result}

C√ìDIGO VULNERABLE (referencia):
L√≠nea {self.context.vulnerability.get('line')} en {self.context.vulnerability.get('file')}

TAREA: Dise√±ar un test conceptual que demuestre la vulnerabilidad.

El test debe:
1. SER REPRODUCIBLE: Otros ingenieros puedan ejecutarlo
2. DEMOSTRAR EL RIESGO: Mostrar el impacto de seguridad
3. SER ESPEC√çFICO: Apuntar a la vulnerabilidad exacta
4. INCLUIR EXPECTATIVAS: Qu√© resultado esperamos (crash, data leak, etc.)

Formato de respuesta JSON:
{{
  "test_design": {{
    "test_concept": "descripci√≥n del test",
    "steps_to_reproduce": ["paso 1", "paso 2"],
    "expected_behavior_without_fix": "qu√© pasa sin fix",
    "verification_method": "c√≥mo verificar la vulnerabilidad",
    "tools_needed": ["compilador", "debugger", "etc."]
  }}
}}"""
        
        try:
            response = await self.llm.generate_fix({
                'vulnerability': self.context.vulnerability,
                'analysis': self.context.analysis_result,
                'custom_prompt': test_prompt
            })
            
            import json
            test_design = json.loads(list(response.values())[0])
            self.context.test_design = test_design
            
            print(f"   ‚úÖ [{self.llm_name}] Test dise√±ado")
            
        except Exception as e:
            print(f"   ‚ùå [{self.llm_name}] Error dise√±ando test: {e}")
            raise
    
    async def _state_fix_designing(self):
        """Estado 3: Dise√±ar el fix para la vulnerabilidad."""
        print(f"   üîß [{self.llm_name}] Dise√±ando fix...")
        
        # Leer el archivo real del worktree
        file_path = self.context.current_file
        full_path = f"{self.context.worktree_dir}/{file_path}"
        
        try:
            with open(full_path, 'r') as f:
                original_code = f.read()
        except FileNotFoundError:
            print(f"   ‚ö†Ô∏è  [{self.llm_name}] Archivo no encontrado: {file_path}")
            # Usar c√≥digo del contexto
            original_code = self.context.vulnerability.get('code_snippet', '')
        
        fix_prompt = f"""DISE√ëO DE FIX PARA VULNERABILIDAD DE SEGURIDAD

AN√ÅLISIS PREVIO:
{self.context.analysis_result}

TEST CONCEPTUAL (para validar):
{self.context.test_design}

C√ìDIGO ORIGINAL (vulnerable):
```
{original_code}
```

TAREA: Generar el c√≥digo FIX que resuelva la vulnerabilidad.

REQUISITOS DEL FIX:
1. RESOLVER LA VULNERABILIDAD: Eliminar el riesgo de seguridad
2. MANTENER FUNCIONALIDAD: No romper el comportamiento original
3. SEGUIR ESTILO DE C√ìDIGO: Coherencia con el c√≥digo base
4. A√ëADIR COMENTARIOS: Explicar el fix de seguridad
5. SER M√çNIMO: Cambiar solo lo necesario

Formato de respuesta:
- Si solo un archivo: c√≥digo completo con fix
- Si m√∫ltiples archivos: JSON con {{"fixed_files": {{"archivo": "c√≥digo"}}}}
"""
        
        try:
            response = await self.llm.generate_fix({
                'vulnerability': self.context.vulnerability,
                'analysis': self.context.analysis_result,
                'test_design': self.context.test_design,
                'original_code': original_code,
                'custom_prompt': fix_prompt
            })
            
            self.context.fix_code = response
            
            print(f"   ‚úÖ [{self.llm_name}] Fix dise√±ado")
            
        except Exception as e:
            print(f"   ‚ùå [{self.llm_name}] Error dise√±ando fix: {e}")
            raise
    
    async def _state_documenting(self):
        """Estado 4: Documentar la soluci√≥n completa."""
        print(f"   üìù [{self.llm_name}] Documentando soluci√≥n...")
        
        doc_prompt = f"""DOCUMENTACI√ìN COMPLETA DE SOLUCI√ìN DE SEGURIDAD

RESUMEN DE TRABAJO:
1. AN√ÅLISIS: {self.context.analysis_result}
2. TEST CONCEPTUAL: {self.context.test_design}
3. FIX: {self.context.fix_code}

TAREA: Crear documentaci√≥n para:
1. Desarrolladores (c√≥mo aplicar el fix)
2. Revisores de seguridad (por qu√© es seguro)
3. Futuros mantenedores (qu√© hace el fix)

INCLUIR:
- Descripci√≥n de la vulnerabilidad
- Explicaci√≥n del fix
- Instrucciones para validar
- Consideraciones de rendimiento
- Posibles efectos secundarios
- Referencias (CWE, best practices)

Formato de respuesta JSON:
{{
  "documentation": {{
    "summary": "resumen ejecutivo",
    "vulnerability_details": "detalles t√©cnicos",
    "fix_explanation": "explicaci√≥n del fix",
    "validation_instructions": "c√≥mo validar",
    "performance_impact": "impacto en rendimiento",
    "references": ["CWE-XXX", "OWASP", "etc."],
    "author": "{self.llm_name}"
  }}
}}"""
        
        try:
            response = await self.llm.generate_fix({
                'vulnerability': self.context.vulnerability,
                'analysis': self.context.analysis_result,
                'test_design': self.context.test_design,
                'fix_code': self.context.fix_code,
                'custom_prompt': doc_prompt
            })
            
            import json
            documentation = json.loads(list(response.values())[0])
            self.context.documentation = documentation
            
            print(f"   ‚úÖ [{self.llm_name}] Documentaci√≥n completada")
            
        except Exception as e:
            print(f"   ‚ùå [{self.llm_name}] Error documentando: {e}")
            raise
    
    async def _state_council_discussing(self, branch_name: str):
        """Estado 5: Participar en discusi√≥n del consejo."""
        print(f"   üèõÔ∏è  [{self.llm_name}] Participando en consejo...")
        
        # Preparar presentaci√≥n para el consejo
        presentation = {
            'llm_name': self.llm_name,
            'analysis': self.context.analysis_result,
            'test_design': self.context.test_design,
            'fix_code': self.context.fix_code,
            'documentation': self.context.documentation,
            'branch': branch_name
        }
        
        # Enviar al consejo (si existe)
        if self.council:
            feedback = await self.council.present_solution(
                llm_name=self.llm_name,
                solution=presentation,
                vulnerability=self.context.vulnerability
            )
            self.context.council_feedback = feedback
    
    def _compile_results(self, branch_name: str) -> Dict[str, Any]:
        """Compila todos los resultados del workflow."""
        return {
            'llm_name': self.llm_name,
            'state': self.state.name,
            'branch': branch_name,
            'worktree_dir': self.context.worktree_dir,
            'start_time': self.start_time,
            'end_time': self.end_time,
            'duration': (self.end_time - self.start_time).total_seconds() if self.end_time else None,
            'artifacts': {
                'vulnerability': self.context.vulnerability,
                'analysis': self.context.analysis_result,
                'test_design': self.context.test_design,
                'fix_code': self.context.fix_code,
                'documentation': self.context.documentation,
                'council_feedback': self.context.council_feedback
            },
            'state_history': self.state_history,
            'errors': self.context.errors
        }
    
    def get_status(self) -> Dict[str, Any]:
        """Obtiene el estado actual de la m√°quina."""
        return {
            'llm_name': self.llm_name,
            'current_state': self.state.name,
            'context': {
                'current_file': self.context.current_file if self.context else None,
                'has_analysis': bool(self.context and self.context.analysis_result),
                'has_test_design': bool(self.context and self.context.test_design),
                'has_fix': bool(self.context and self.context.fix_code),
                'has_documentation': bool(self.context and self.context.documentation)
            },
            'history_length': len(self.state_history),
            'errors': self.context.errors if self.context else []
        }
```

## üîÑ **INTEGRACI√ìN CON EL CONSEJO:**

```python
# src/core/llm_council_enhanced.py
class EnhancedLLMCouncil(LLMCouncil):
    """Consejo extendido con m√°quinas de estados por LLM."""
    
    def __init__(self, config_path: Optional[str] = None):
        super().__init__(config_path)
        self.state_machines: Dict[str, LLMStateMachine] = {}
        self.solutions: Dict[str, List[Dict]] = {}  # vulnerability_id -> solutions
    
    async def initialize_with_state_machines(self, worktree_manager):
        """Inicializa consejo con m√°quinas de estados para cada LLM."""
        await self.initialize_council()
        
        print(f"üèóÔ∏è  Creando m√°quinas de estados para {len(self.llms)} LLMs...")
        
        for llm in self.llms:
            state_machine = LLMStateMachine(
                llm=llm,
                llm_name=llm.name,
                worktree_manager=worktree_manager,
                council=self
            )
            self.state_machines[llm.name] = state_machine
        
        print(f"‚úÖ {len(self.state_machines)} m√°quinas de estados creadas")
        return self.state_machines
    
    async def orchestrate_vulnerability_fix(self, vulnerability: Dict[str, Any], 
                                           worktree_manager) -> Dict[str, Any]:
        """
        Orquesta el fix completo de una vulnerabilidad usando m√°quinas de estados.
        
        Returns:
            Resultados de todos los LLMs
        """
        vulnerability_id = vulnerability.get('rule_id', 'unknown')
        print(f"\nüèõÔ∏è  CONSEJO ORQUESTANDO FIX PARA: {vulnerability_id}")
        
        # Crear worktrees y branches para cada LLM
        llm_tasks = {}
        results = {}
        
        for llm_name, state_machine in list(self.state_machines.items())[:3]:  # Limitar a 3 para prueba
            try:
                # Crear worktree para este LLM
                worktree_dir, branch_name = worktree_manager.create_worktree_for_llm(
                    llm_name=llm_name,
                    issue_id=vulnerability_id
                )
                
                # Ejecutar workflow completo
                task = asyncio.create_task(
                    state_machine.execute_workflow(vulnerability, worktree_dir, branch_name),
                    name=f"workflow_{llm_name}_{vulnerability_id}"
                )
                llm_tasks[llm_name] = task
                
            except Exception as e:
                print(f"‚ùå Error preparando {llm_name}: {e}")
                results[llm_name] = {'error': str(e), 'success': False}
        
        # Ejecutar workflows en paralelo
        if llm_tasks:
            print(f"üöÄ Ejecutando {len(llm_tasks)} workflows en paralelo...")
            
            task_results = await asyncio.gather(
                *llm_tasks.values(),
                return_exceptions=True
            )
            
            # Procesar resultados
            for llm_name, result in zip(llm_tasks.keys(), task_results):
                if isinstance(result, Exception):
                    results[llm_name] = {
                        'success': False,
                        'error': str(result),
                        'artifacts': None
                    }
                else:
                    results[llm_name] = {
                        'success': True,
                        'error': None,
                        'artifacts': result
                    }
                    
                    # Aplicar fix al worktree si existe
                    if result.get('artifacts', {}).get('fix_code'):
                        try:
                            worktree_info = worktree_manager.worktrees.get(result['branch'])
                            if worktree_info:
                                commit_hash = worktree_manager.apply_llm_fix(
                                    branch_name=result['branch'],
                                    fixed_files=result['artifacts']['fix_code'],
                                    llm_name=llm_name
                                )
                                
                                # Hacer push
                                success, push_url = worktree_manager.push_to_github(result['branch'])
                                
                                if success:
                                    # Crear PR
                                    pr_url = worktree_manager.create_pull_request(result['branch'])
                                    results[llm_name]['pr_url'] = pr_url
                                    results[llm_name]['commit'] = commit_hash[:8]
                                    
                                    print(f"‚úÖ {llm_name}: PR creado - {pr_url}")
                        except Exception as e:
                            print(f"‚ö†Ô∏è  Error aplicando fix de {llm_name}: {e}")
        
        # Guardar soluciones para discusi√≥n del consejo
        self.solutions[vulnerability_id] = results
        
        # Realizar sesi√≥n de consejo
        await self._council_session(vulnerability_id)
        
        return results
    
    async def _council_session(self, vulnerability_id: str):
        """Sesi√≥n de consejo donde los LLMs discuten soluciones."""
        if vulnerability_id not in self.solutions:
            return
        
        solutions = self.solutions[vulnerability_id]
        successful_solutions = {k: v for k, v in solutions.items() if v.get('success')}
        
        if len(successful_solutions) < 2:
            print("‚ö†Ô∏è  No hay suficientes soluciones exitosas para discusi√≥n del consejo")
            return
        
        print(f"\nüèõÔ∏è  SESI√ìN DE CONSEJO PARA {vulnerability_id}")
        print(f"   Participantes: {list(successful_solutions.keys())}")
        
        # Por ahora, solo mostrar resumen
        # En el futuro: implementar discusi√≥n real entre LLMs
        for llm_name, solution in successful_solutions.items():
            artifacts = solution.get('artifacts', {})
            print(f"\n   üë§ {llm_name}:")
            print(f"      üìä An√°lisis: {artifacts.get('analysis', {}).get('analysis', {}).get('what', 'N/A')[:100]}...")
            print(f"      üß™ Test: {artifacts.get('test_design', {}).get('test_design', {}).get('test_concept', 'N/A')[:100]}...")
            if solution.get('pr_url'):
                print(f"      üîó PR: {solution['pr_url']}")
```

## üöÄ **NUEVO COMANDO PARA MA√ëANA:**

```bash
# Comando objetivo
python tdh_unified.py council fix-with-states \
  "https://github.com/alonsoir/test-zeromq-c-.git" \
  --llm-count 3

# Flujo:
# 1. Clona repo
# 2. Ejecuta SAST real
# 3. Selecciona vulnerabilidad real
# 4. Crea consejo con m√°quinas de estados
# 5. Cada LLM ejecuta su workflow (4 estados)
# 6. Consejo discute resultados
# 7. Crea PRs comparativos
```

## üìã **PLAN DE IMPLEMENTACI√ìN PARA MA√ëANA:**

### **1. Ma√±ana temprano:**
- Crear `llm_state_machine.py`
- Crear `llm_council_enhanced.py`
- Actualizar `tdh_unified.py` con nuevo comando

### **2. Medio d√≠a:**
- Integrar SAST real ‚Üí m√°quinas de estados
- Probar flujo completo con vulnerabilidad real

### **3. Tarde:**
- Implementar discusi√≥n b√°sica del consejo
- Mejorar prompts y manejo de errores
- Documentar resultados

## üéØ **LO QUE LOGRA ESTE DISE√ëO:**

1. **Cada LLM tiene su propio estado** ‚Üí Sabemos exactamente qu√© est√° haciendo
2. **Flujo estructurado** ‚Üí 4 fases claras con artefactos espec√≠ficos
3. **Artefactos generados** ‚Üí An√°lisis, test conceptual, fix, documentaci√≥n
4. **Consejo coordinado** ‚Üí Todos participan en discusi√≥n
5. **PRs con documentaci√≥n completa** ‚Üí No solo c√≥digo, sino explicaci√≥n

## üîÆ **EVOLUCI√ìN FUTURA:**

Una vez que esto funcione, podemos a√±adir:
- **Ejecuci√≥n real de tests** (compilaci√≥n, ejecuci√≥n)
- **Validaci√≥n autom√°tica** de fixes
- **M√©tricas de calidad** comparativas
- **Sistema de votaci√≥n** avanzado

**¬øTe parece bien este dise√±o?** Ma√±ana lo implementamos paso a paso, comenzando con la m√°quina de estados b√°sica y luego integrando con SAST real.